
-----

# Laborat√≥rio de Estudos de Big Data: Stack Hadoop & Spark com Docker

## üéØ Objetivo do Projeto

Este reposit√≥rio documenta a jornada de constru√ß√£o de um ecossistema completo de Big Data, do zero, utilizando Docker Compose. O objetivo principal √© o aprendizado pr√°tico e aprofundado dos componentes fundamentais de uma arquitetura de dados moderna, desde o armazenamento distribu√≠do at√© o processamento em larga escala para ETL, Machine Learning e visualiza√ß√£o de dados.

O projeto foi constru√≠do de forma incremental, pe√ßa por pe√ßa, para permitir o entendimento das depend√™ncias e da intera√ß√£o entre cada servi√ßo, replicando em um ambiente local os desafios de configura√ß√£o e depura√ß√£o (e compatibilidade de vers√µes) encontrados em sistemas de produ√ß√£o.

## üèõÔ∏è Arquitetura da Solu√ß√£o

A arquitetura implementada segue o padr√£o de um Data Lake moderno, com camadas bem definidas para ingest√£o, armazenamento, processamento e consumo de dados.

  * **Camada de Armazenamento (Data Lake):** Utiliza **HDFS** para armazenar dados em seu formato bruto (`raw`) e processado (`processed`).
  * **Camada de Gerenciamento de Recursos:** O **YARN** atua como o "sistema operacional" do cluster, gerenciando os recursos de CPU e mem√≥ria para as aplica√ß√µes.
  * **Camada de Cat√°logo de Metadados:** O **Apache Hive**, atrav√©s do **Hive Metastore** (com backend em PostgreSQL), serve como um cat√°logo central para todos os dados do Data Lake.
  * **Camada de Processamento (ETL & ML):** O **Apache Spark** √© o motor principal para o processamento de dados, executando pipelines de ETL e preparando dados para Machine Learning.
  * **Camada de Servi√ßo (Serving Layer):** Para acesso de baixa lat√™ncia, utilizamos **Apache HBase**. Para habilitar o acesso SQL, criamos **imagens Docker customizadas** (`thedarklordottm/hbase-phoenix-*`) que embutem o **Apache Phoenix** diretamente nos servi√ßos do HBase.
  * **Camada de Visualiza√ß√£o (BI):** O **Apache Superset** √© usado para criar dashboards e visualiza√ß√µes. Para garantir a conectividade com todas as nossas fontes de dados (PostgreSQL, Hive, Phoenix, MySQL, etc.), foi criada uma **imagem Docker customizada** do Superset que inclui bibliotecas essenciais como `Pillow` (para exporta√ß√£o de imagens), `psycopg2`, `sqlalchemy-phoenix`, `mysqlclient`, `pyhive` e outras.
  * **Camada de Automa√ß√£o:** O **n8n** √© utilizado para a automa√ß√£o de workflows e integra√ß√£o entre servi√ßos.
  * **Camada de Orquestra√ß√£o:** O **Apache Airflow** (com backend em MySQL) est√° configurado para agendar, executar e monitorar os pipelines de dados de forma program√°tica.

## üõ†Ô∏è Tecnologias Utilizadas (Stack)

| Categoria | Componente | Status |
| :--- | :--- | :--- |
| **Infraestrutura** | Docker & Docker Compose | ‚úÖ **Implementado** |
| **Armazenamento (Storage)** | HDFS (Namenode, Datanode) | ‚úÖ **Implementado** |
| **Gerenciamento de Recursos** | YARN (ResourceManager, NodeManager) | ‚úÖ **Implementado** |
| **Cat√°logo de Metadados** | Hive Metastore + PostgreSQL | ‚úÖ **Implementado** |
| **Acesso SQL (Data Lake)** | Hive Server | ‚úÖ **Implementado** |
| **Processamento Distribu√≠do** | Apache Spark (Master, Worker) | ‚úÖ **Implementado** |
| **Coordena√ß√£o** | Apache ZooKeeper | ‚úÖ **Implementado** |
| **Banco NoSQL (Serving Layer)**| Apache HBase (Master, RegionServer) | ‚úÖ **Implementado** |
| **Camada SQL para NoSQL** | Apache Phoenix (embutido no HBase) | ‚úÖ **Implementado** |
| **Orquestra√ß√£o de Pipeline** | Apache Airflow + MySQL | ‚úÖ **Implementado** |
| **Visualiza√ß√£o de Dados (BI)**| Apache Superset | ‚úÖ **Implementado** |
| **Automa√ß√£o de Workflow** | n8n | ‚úÖ **Implementado** |


## üöÄ Como Executar o Projeto

1.  **Pr√©-requisitos:**

      * Docker e Docker Compose instalados.
      * Git para clonar o reposit√≥rio.

2.  **Configura√ß√£o:**

      * Clone este reposit√≥rio: `git clone <URL_DO_SEU_REPOSIT√ìRIO>`
      * Navegue para a pasta do projeto: `cd <NOME_DA_PASTA>`
      * Certifique-se de que todos os arquivos de configura√ß√£o (`hadoop.env`, `hive-conf/hive-site.xml`, `hbase-conf/hbase-site.xml`, `zoo-conf/zoo.cfg`) est√£o presentes.

3.  **Execu√ß√£o:**

      * Para subir todo o ambiente em segundo plano, execute:
        ```bash
        docker-compose up -d
        ```
      * Para verificar o status dos servi√ßos:
        ```bash
        docker-compose ps
        ```

4.  **Acessando as Interfaces Web:**

      * **HDFS (Namenode):** `http://localhost:9870`
      * **YARN (ResourceManager):** `http://localhost:8088`
      * **Spark (Master):** `http://localhost:8081`
      * **Hive (Server2 UI):** `http://localhost:10002`
      * **HBase (Master):** `http://localhost:16010`
      * **HBase (RegionServer):** `http://localhost:16030`
      * **Superset (BI):** `http://localhost:8090`
      * **n8n (Automa√ß√£o):** `http://localhost:5678`
      * **Airflow (Orquestra√ß√£o):** `http://localhost:8080` (Quando reativado)

## ‚ö° Exemplos de Uso

### Conectar ao Hive via Beeline

```bash
# Entrar no cont√™iner do hive-server
docker exec -it hive-server /bin/bash

# Conectar ao servi√ßo
beeline -u jdbc:hive2://localhost:10000
```

### Submeter um Job PySpark

```bash
# Submeter o script 'meu_job.py' localizado na pasta 'jobs'
docker exec spark-master /spark/bin/spark-submit /opt/spark/jobs/meu_job.py
```

### Conectar ao HBase/Phoenix via SQL

O HBase por si s√≥ n√£o fala SQL. Nossas imagens customizadas usam o Apache Phoenix para criar uma camada de acesso SQL.

**1. Iniciar o Phoenix Query Server (PQS):**
Este servi√ßo atua como um "tradutor" que recebe SQL e o converte em chamadas HBase. Ele precisa ser iniciado manualmente:

```bash
docker exec -d hbase-master /opt/phoenix/bin/queryserver.py start
```


**2. Conectar via Superset:**
Na UI do Superset, adicione um novo banco de dados usando a seguinte string de conex√£o (SQLAlchemy URI):
`phoenix://hbase-master:8765/`

**3. (Alternativo) Conectar via Cliente de Linha de Comando:**

```bash
# Entrar no cont√™iner do hbase-master
docker exec -it hbase-master /bin/bash

# Iniciar o cliente sqlline (requer python2 na imagem)
python2 /opt/phoenix/bin/sqlline.py zookeeper:2181
```

### Inicializando o Apache Superset (Primeira Vez)

O Superset n√£o cria um usu√°rio administrador automaticamente. Ap√≥s subir os servi√ßos `superset-db` e `superset-redis`, voc√™ precisa rodar os seguintes comandos **em ordem** para inicializar a aplica√ß√£o:

```bash
# 1. Atualiza o banco de dados interno do Superset
docker-compose run --rm superset superset db upgrade

# 2. Cria um usu√°rio administrador (siga os prompts interativos)
docker-compose run --rm superset superset fab create-admin

# 3. Inicializa as permiss√µes e pap√©is padr√£o
docker-compose run --rm superset superset init

# 4. Inicie o servi√ßo principal
docker-compose up -d superset
```


-----

*Este projeto √© um ambiente de estudos e n√£o se destina ao uso em produ√ß√£o.*